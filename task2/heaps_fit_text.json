{
  "dataset": "poems_translated.parquet",
  "rows": 846,
  "tokenizer": "simple-space (task1_tokenize.py)",
  "step": 1000,
  "total_tokens": 132975,
  "unique_types": 50232,
  "k": 2.305737586635824,
  "beta": 0.8465996707590482,
  "r2": 0.9998122016071375,
  "num_points": 133
}